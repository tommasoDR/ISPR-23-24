{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSJeGm-d6Rov"
      },
      "source": [
        "# Assignment 5 - Di Riccio Tommaso - 18/05/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBN84K106Row"
      },
      "source": [
        "## BidirectionalLSTM class\n",
        "To address the task of predicting ratings from review text, I opted for a network architecture based on LSTM units. LSTMs are particularly well-suited for sequential data processing, allowing them to better capture long-term dependencies present in long reviews.\n",
        "\n",
        "As the first computational layer in the chosen architecture, I used a bidirectional layer featuring LSTM units. This layer processes the input sequence in both forward and backward directions, enabling the model to capture dependencies from both past and future contexts, thereby enhancing its understanding of the review text.\n",
        "\n",
        "The second layer of the model is a standard LSTM layer used to further process the information gathered from the bidirectional layer. I chose a plain LSTM layer instead of a bidirectional one to have a deeper network without introducing excessive computational complexity.\n",
        "\n",
        "For the output layer, softmax is used, treating the problem as a classification task with 9 classes (i.e., ratings from 1 to 9).\n",
        "\n",
        "This architecture is encapsulated in the <code>BidirectionalLSTM</code> class. This class allows instantiation of the Keras model by passing the pre-trained embeddings matrix, the number of units to use, and the dropout parameter to control regularization. Additionally, the class wraps the most useful methods of a Keras model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEYpu2sq6Rox"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "\n",
        "class BidirectionalLSTM:\n",
        "    def __init__(self, num_words, embedding_dim, embedding_matrix, lstm_units=10, dropout=0.0, recurrent_dropout=0.0):\n",
        "            \"\"\"\n",
        "            Constructor for the Bidirectional LSTM class.\n",
        "\n",
        "            :param num_words: The size of the vocabulary.\n",
        "            :param embedding_dim: The dimensionality of the embedding vectors.\n",
        "            :param embedding_matrix: The pretrained embedding matrix.\n",
        "            :param lstm_units: The number of LSTM units.\n",
        "            :param dropout: The dropout rate.\n",
        "            :param recurrent_dropout: The recurrent dropout rate.\n",
        "            \"\"\"\n",
        "            # Define the model architecture\n",
        "            bidirLSTM = keras.models.Sequential()\n",
        "            # Add an input layer\n",
        "            bidirLSTM.add(keras.layers.Input(shape=(None,), dtype=\"int32\"))\n",
        "            # Add an embedding layer to convert input sequences to dense vectors\n",
        "            bidirLSTM.add(\n",
        "                keras.layers.Embedding(\n",
        "                    input_dim=num_words,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=embedding_dim,\n",
        "                    trainable=False,\n",
        "                    mask_zero=True,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Add a Bidirectional LSTM layer\n",
        "            bidirLSTM.add(keras.layers.Bidirectional(keras.layers.LSTM(units=lstm_units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout)))\n",
        "\n",
        "            # Add a LSTM layer\n",
        "            bidirLSTM.add(keras.layers.LSTM(units=lstm_units, return_sequences=False, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
        "\n",
        "            # Add a dense output layer\n",
        "            bidirLSTM.add(keras.layers.Dense(units=9, activation=\"softmax\"))\n",
        "\n",
        "            # Optimizer\n",
        "            optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "            # Compile the model\n",
        "            bidirLSTM.compile(\n",
        "                loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
        "            )\n",
        "\n",
        "            self._bidirLSTM = bidirLSTM\n",
        "\n",
        "\n",
        "    def fit(self, inputs, targets, **kwargs):\n",
        "        \"\"\"\n",
        "        Fit the model to the given inputs and targets.\n",
        "\n",
        "        :param inputs: the input data.\n",
        "        :param targets: the target data.\n",
        "        :param kwargs: additional arguments to pass to the fit method.\n",
        "        \"\"\"\n",
        "        self._bidirLSTM.fit(inputs, targets, **kwargs)\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"\n",
        "        Print a summary of the model.\n",
        "        \"\"\"\n",
        "        self._bidirLSTM.summary()\n",
        "\n",
        "    def evaluate(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        Evaluate the model.\n",
        "\n",
        "        :param inputs: the data to evaluate the model on\n",
        "        :param targets: the target values\n",
        "        :return: the score of the model\n",
        "        \"\"\"\n",
        "        return self._bidirLSTM.evaluate(inputs, targets)\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"\n",
        "        Make prediction over data.\n",
        "\n",
        "        :param data: the data to predict\n",
        "        :return: the predicted values\n",
        "        \"\"\"\n",
        "        return self._bidirLSTM.predict(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H_zNUeH6Roy"
      },
      "source": [
        "## Utils methods\n",
        "\n",
        "In the following code are present all the utils methods used to preprocess the text. In particular:\n",
        "- **<code>Lowercase</code>**: this method converts all text to lowercase, ensuring uniformity in the text data.\n",
        "\n",
        "- **<code>Remove Stopwords</code>**: this method removes common stopwords from the text using NLTK's stopwords.\n",
        "\n",
        "- **<code>Remove Non-Alphanumeric Characters</code>**: this method removes non-alphanumeric characters from the text, retaining only letters, numbers, and spaces.\n",
        "\n",
        "- **<code>Remove Contractions</code>**: this method expands contractions in the text, replacing them with their full forms. For example, \"don't\" becomes \"do not\".\n",
        "\n",
        "- **<code>Tokenize</code>**: this method tokenizes the text data, converting words into integers, and pads sequences to ensure uniform length for input into neural networks. The vocabulary created is returned. It utilizes TensorFlow's Tokenizer and pad_sequences functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpyDF9yS7c9h",
        "outputId": "91514c39-71b2-4d49-885e-9a38da2dd4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI9-JZsa6Roz",
        "outputId": "9bd79a0d-1289-4631-9b55-5144341dc10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import contractions\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if (word not in stop_words)]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def remove_non_alphanumeric(text):\n",
        "    return ''.join([char if char.isalnum() or char.isspace() else ' ' for char in text ])\n",
        "\n",
        "def remove_contractions(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [contractions.fix(word)for word in words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "\n",
        "def tokenize(sequences_train, sequences_test, vocab_size, max_sequence_length):\n",
        "    # Tokenization\n",
        "    vocab_size = vocab_size\n",
        "    tokenizer = Tokenizer(num_words = vocab_size, oov_token=None)\n",
        "    concat = pd.concat([sequences_train, sequences_test])\n",
        "    tokenizer.fit_on_texts(concat)\n",
        "    sequences_as_integers_train = tokenizer.texts_to_sequences(sequences_train)\n",
        "    sequences_as_integers_test = tokenizer.texts_to_sequences(sequences_test)\n",
        "\n",
        "    # Padding\n",
        "    padding_type = 'post'\n",
        "    truncation_type = 'post'\n",
        "    x_train = pad_sequences(sequences_as_integers_train, maxlen=max_sequence_length, padding=padding_type, truncating=truncation_type)\n",
        "    x_test = pad_sequences(sequences_as_integers_test, maxlen=max_sequence_length, padding=padding_type, truncating=truncation_type)\n",
        "\n",
        "    return x_train, x_test, tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Fp_F0f6Roz"
      },
      "source": [
        "## Preprocessing and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0wPmEtI6Roz",
        "outputId": "e332dbbd-0f57-402c-f37f-a47a42cb9e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Y32ppP0cTXEiXejPaIglZaTbUuOf3xyS\n",
            "To: /content/Airline_Reviews.csv\n",
            "\r  0% 0.00/20.5M [00:00<?, ?B/s]\r 69% 14.2M/20.5M [00:00<00:00, 140MB/s]\r100% 20.5M/20.5M [00:00<00:00, 168MB/s]\n"
          ]
        }
      ],
      "source": [
        "# DATASET LOAD\n",
        "import pandas as pd\n",
        "\n",
        "!gdown 1Y32ppP0cTXEiXejPaIglZaTbUuOf3xyS\n",
        "\n",
        "dataset = pd.read_csv(\"/content/Airline_Reviews.csv\")\n",
        "\n",
        "dataset = dataset[['Review', 'Overall_Rating']]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step in the data preprocessing pipeline is to remove all the entries in the dataset that have missing or non-numeric values as rating.\n",
        "\n",
        "Then, the remaining entries undergo further preprocessing steps using the utility methods <code>lowercase</code>, <code>remove_contractions</code> and <code>remove_non_alphanumeric</code>.\n",
        "\n",
        "After running some tests, I decided to keep the stopwords as they provide important context necessary to maintain an accurate representation of the original text. For instance, stopwords like \"not\" can completely reverse the polarity of a sentence.\n",
        "\n",
        "Also stemming was used but then it was removed in favor of using pretrained embeddings."
      ],
      "metadata": {
        "id": "JNo_zh9d7XlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WrGkf5g6Ro0"
      },
      "outputs": [],
      "source": [
        "# DATA PREPROCESSING\n",
        "\n",
        "# Discard entry with non-numeric rating\n",
        "mask = dataset[\"Overall_Rating\"].apply(lambda x: x.isnumeric())\n",
        "dataset = dataset[mask]\n",
        "\n",
        "# Lowercasing\n",
        "dataset[\"Review\"] = dataset[\"Review\"].apply(lowercase)\n",
        "\n",
        "# Remove contractions\n",
        "dataset[\"Review\"] = dataset[\"Review\"].apply(remove_contractions)\n",
        "\n",
        "# Remove non-alphanumeric characters\n",
        "dataset[\"Review\"] = dataset[\"Review\"].apply(remove_non_alphanumeric)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After preprocessing, the target value are one-hot encoded and the dataset is split into train and test set (90% - 10%)."
      ],
      "metadata": {
        "id": "VzixgLGT-li6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqFWfWWi6Ro0"
      },
      "outputs": [],
      "source": [
        "# DATA SPLIT\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = dataset[\"Review\"]\n",
        "\n",
        "# Vote one-hot encoding\n",
        "y = pd.get_dummies(dataset[\"Overall_Rating\"])\n",
        "\n",
        "# Split dataset in train and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, all the reviews are tokenized and converted into sequences of integers, with each integer representing the index of the corresponding words in the constructed vocabulary. I chose a vocabulary size of 10,000 because this has been shown to strike a good balance between capturing sufficient word diversity and ignoring very rare words that are not be useful for the task.\n",
        "\n",
        "The sequences are then padded or truncated to have a fixed length of 200 elements. This choice strikes a good balance between representing the majority of reviews in their entirety and avoiding excessive padding that could lead to increased memory usage."
      ],
      "metadata": {
        "id": "rUDO7hIa_W3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je5Wdhi86Ro0"
      },
      "outputs": [],
      "source": [
        "# TOKENIZATION AND VOCABULARY CREATION\n",
        "x_train, x_test, word_index = tokenize(x_train, x_test, vocab_size=10000, max_sequence_length=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the vocabulary has been built, it is possible to create the matrix with the pretrained embeddings. The decision to use pretrained embeddings is to greatly reduce the number of trainable parameters and improve the model performance through transfer learning. In particular, GloVe embeddings of size 200 are used.\n",
        "\n",
        "It is possible to see that pretrained embeddings exist for 78.34% of the tokens found in the preprocessed review text. While 21.66% of tokens lack of pretrained embeddings, this isn't a significant concern. In fact, given the utilization of a vocabulary size of 10,000, it's highly likely that these tokens without a pretrained embedding will be excluded from the input sequences. After evaluation, I observed that they mainly consist of misspelled words, non-English terms (most Italian and Arabic) and acronyms. Therefore their absence is not a major issue for the model's performance."
      ],
      "metadata": {
        "id": "gb3EgeULCiym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73-b_urZ6Ro1",
        "outputId": "0a6ad025-a23e-4cdb-c264-b4d44f41e6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Qf6gBW6omI8Fwvew6H1wfvce9qXXuPd-\n",
            "From (redirected): https://drive.google.com/uc?id=1Qf6gBW6omI8Fwvew6H1wfvce9qXXuPd-&confirm=t&uuid=1dedfa84-2e46-4f99-8d73-3463402cb9a5\n",
            "To: /content/glove.6B.200d.txt\n",
            "100% 693M/693M [00:05<00:00, 126MB/s]\n",
            "Percentage of words found in pre-trained embeddings:  78.34 %\n"
          ]
        }
      ],
      "source": [
        "# PRE-TRAINED EMBEDDINGS\n",
        "import numpy as np\n",
        "\n",
        "!gdown 1Qf6gBW6omI8Fwvew6H1wfvce9qXXuPd-\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(\"/content/glove.6B.200d.txt\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "# EMBEDDING MATRIX\n",
        "num_words = len(word_index) + 1\n",
        "embedding_dim = 200\n",
        "\n",
        "found = 0\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        found += 1\n",
        "\n",
        "print(\"Percentage of words found in pre-trained embeddings: \", np.around(found/num_words*100,2), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the disadvantages of bidirectional LSTM is the time needed for training. Given the limited hardware at my disposal (a laptop without a dedicated GPU), I initially attempted networks with a limited number of LSTM units (i.e., the dimension of the inner cells in LSTM). After some trials, I realized that this approach wouldn't be feasible, so I switched to using Google Colab to leverage the provided GPU. The improvement in training performance was significant, allowing me to test networks with many more units.\n",
        "\n",
        "Unfortunately, it still wasn't possible to conduct a grid search due to the limitation that Colab terminates the session if the user does not interact with the page for a few minutes. Therefore, I started manually testing different configurations varying:\n",
        "- the BidirectionalLSTM class parameters\n",
        "- vocabulary size\n",
        "- pretrained embeddings dimension\n",
        "- maximum sequence length\n",
        "- learning rate\n",
        "- the parameters of the fit function\n",
        "- the optimizer used (Adam and SGD)\n",
        "- the preprocessing pipeline\n",
        "\n",
        "In the end, one of the best-performing networks (although some were very close) is the one summarized in the following."
      ],
      "metadata": {
        "id": "W6yX6iJ7Ida_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyQrJxKs6Ro1",
        "outputId": "71cbdc90-ee87-4884-856b-1076accfddcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 200)         6647800   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, None, 150)         165600    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 75)                67800     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 9)                 684       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6881884 (26.25 MB)\n",
            "Trainable params: 234084 (914.39 KB)\n",
            "Non-trainable params: 6647800 (25.36 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "314/314 [==============================] - 24s 37ms/step - loss: 1.5171\n",
            "Epoch 2/15\n",
            "314/314 [==============================] - 12s 38ms/step - loss: 1.4125\n",
            "Epoch 3/15\n",
            "314/314 [==============================] - 12s 39ms/step - loss: 1.3523\n",
            "Epoch 4/15\n",
            "314/314 [==============================] - 13s 40ms/step - loss: 1.3202\n",
            "Epoch 5/15\n",
            "314/314 [==============================] - 12s 37ms/step - loss: 1.2883\n",
            "Epoch 6/15\n",
            "314/314 [==============================] - 10s 31ms/step - loss: 1.2694\n",
            "Epoch 7/15\n",
            "314/314 [==============================] - 12s 37ms/step - loss: 1.2519\n",
            "Epoch 8/15\n",
            "314/314 [==============================] - 12s 39ms/step - loss: 1.2385\n",
            "Epoch 9/15\n",
            "314/314 [==============================] - 10s 32ms/step - loss: 1.2258\n",
            "Epoch 10/15\n",
            "314/314 [==============================] - 12s 38ms/step - loss: 1.2170\n",
            "Epoch 11/15\n",
            "314/314 [==============================] - 12s 38ms/step - loss: 1.2012\n",
            "Epoch 12/15\n",
            "314/314 [==============================] - 10s 31ms/step - loss: 1.1906\n",
            "Epoch 13/15\n",
            "314/314 [==============================] - 12s 39ms/step - loss: 1.1793\n",
            "Epoch 14/15\n",
            "314/314 [==============================] - 13s 41ms/step - loss: 1.1659\n",
            "Epoch 15/15\n",
            "314/314 [==============================] - 10s 32ms/step - loss: 1.1556\n"
          ]
        }
      ],
      "source": [
        "# MODEL TRAINING\n",
        "bidirLSTM = BidirectionalLSTM(num_words, embedding_dim, embedding_matrix, lstm_units=75, dropout=0.5, recurrent_dropout=0.0)\n",
        "bidirLSTM.summary()\n",
        "bidirLSTM.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.0, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the result of the final retraining using both training and validation data. Early stopping wasn't used in the final retraining (due to the lack of the validation set) but during model selection 15 epochs allowed to get a good training loss without overfitting thanks to the high value of dropout.\n",
        "\n",
        "It is important to notice that the recurrent dropout (i.e. the dropout between time steps) must be set to zero to allow GPU parallelization. Despite this, regular dropout (i.e. between the layers) was enough to avoid overfit and a better choice than weight regularizers (at least in the tested configurations)."
      ],
      "metadata": {
        "id": "mNme2PjGAhId"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUrSH9xC6Ro1",
        "outputId": "9d69a8a5-ba6f-4591-e990-e5a682cd833f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 1s 14ms/step\n",
            "Distribution of predicted ratings: [1533   58   31   31   52   82  111  187  148]\n",
            "Distribution of actual ratings:  [1159  230  136   86   83   67  119  176  177]\n",
            "Mean difference between predicted and actual ratings:  1.34\n",
            "Ratings predicted exactly:  0.56 %\n"
          ]
        }
      ],
      "source": [
        "# COMPUTE THE MEAN DIFFERENCE BETWEEN THE PREDICTED RATING AND THE ACTUAL RATING\n",
        "import numpy as np\n",
        "\n",
        "# Get the predicted votes from the model\n",
        "predicted_prob = bidirLSTM.predict(x_test)\n",
        "\n",
        "# Convert the predicted votes to the actual vote labels\n",
        "predicted_rating = np.argmax(predicted_prob, axis=1) + 1\n",
        "\n",
        "actual_rating = np.argmax(y_test.values, axis=1) + 1\n",
        "\n",
        "# Compute the difference between the predicted labels and the actual labels\n",
        "difference = np.abs(predicted_rating - actual_rating)\n",
        "\n",
        "# show vote distribution\n",
        "pred_rating = np.zeros(9, dtype=int)\n",
        "for i in predicted_rating:\n",
        "    pred_rating[i-1] += 1\n",
        "\n",
        "act_rating = np.zeros(9, dtype=int)\n",
        "for i in actual_rating:\n",
        "    act_rating[i-1] += 1\n",
        "\n",
        "print(\"Distribution of predicted ratings:\", pred_rating)\n",
        "print(\"Distribution of actual ratings: \", act_rating)\n",
        "\n",
        "# Compute the mean difference and accuracy\n",
        "mean_difference = np.around(np.mean(difference),2)\n",
        "accuracy = np.around(np.sum(difference == 0) / len(difference), 2)\n",
        "\n",
        "print(\"Mean difference between predicted and actual ratings: \", mean_difference)\n",
        "print(\"Ratings predicted exactly: \", accuracy, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final model on the test set is able to exactly predict 56% of ratings. The mean distance between the predicted rating and the actual rating is 1.34.\n",
        "\n",
        "From the distribution of predicted ratings, it can be noticed that the model is slightly biased towards rating 1. This is probably the consequence of having many more entries in the dataset for this rating."
      ],
      "metadata": {
        "id": "_oHneEiqQLDY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEtOpFsi6Ro1",
        "outputId": "1d3c451f-aba1-4d67-d702-2a9dcf293220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual vote: 1\n",
            "Predicted vote: 9\n",
            "Review text: \n",
            "my wife and i recently may 6 2023 flew klm from split through amsterdam to atlanta to return home my wife had a broken and a full leg cast so she could not fly economy the flights attendants on both flights kl kl worked to accommodate both her situation to be as comfortable as possible as well as to allow me to be fairly close to assist i wish to commend all the crew for the assistance in this difficult situation "
          ]
        }
      ],
      "source": [
        "# MAX DIFFERENCE BETWEEN PREDICTED AND ACTUAL RATING\n",
        "max_difference_index = np.argmax(difference)\n",
        "\n",
        "print(\"Actual vote: \" + str(actual_rating[max_difference_index]))\n",
        "print(\"Predicted vote: \" + str(predicted_rating[max_difference_index]))\n",
        "\n",
        "print(\"Review text: \")\n",
        "# Print the review with the maximum difference\n",
        "for i in x_test[max_difference_index]:\n",
        "  if i == 0:\n",
        "    break\n",
        "  print(list(word_index.keys())[list(word_index.values()).index(i)], end=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell I printed the review with the maximum distance between the predicted rating and the actual rating. We can clearly see that there is an error in the review. The text is highly positive, but the actual vote is strongly negative. The same observation can be made for different reviews."
      ],
      "metadata": {
        "id": "dHPSm1O_T874"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Considerations\n",
        "In this dataset and task, there are multiple challenges, such as:\n",
        "- Subjectivity of reviews, especially with ratings in a range from 1 to 9.\n",
        "- Errors in the dataset, like the one previously shown.\n",
        "- Reviews in other languages (like Italian and Arabic), which are difficult to detect and remove during preprocessing.\n",
        "\n",
        "Given these challenges, a more complex and specifically engineered preprocessing of the dataset text would likely provide a higher performance boost than using different architectures. Various approaches have been attempted, such as considering the problem as a regression task, using a CNN after the LSTM (proposed by\n",
        "<a href=\"https://www.sciencedirect.com/science/article/pii/S187705091830601X\">Xiaobin Zhang et al.</a>) or adjusting the number of LSTM layers. However, none of these approaches effectively improved performance on the addressed task."
      ],
      "metadata": {
        "id": "XsXTzj0HdzB7"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}